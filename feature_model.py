"""
Systematic Feature Engineering Framework for SPY Trading
ç³»ç»ŸåŒ–ç‰¹å¾å·¥ç¨‹æ¡†æ¶ - ä¿®æ­£ç‰ˆæœ¬
"""

import numpy as np
import pandas as pd
import talib
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import TimeSeriesSplit
from sklearn.feature_selection import mutual_info_classif
from hmmlearn import hmm
import warnings
warnings.filterwarnings('ignore')

# ==================== æ ¸å¿ƒæ•°æ®ç»“æ„ ====================

@dataclass
class FeatureMetadata:
    """ç‰¹å¾å…ƒæ•°æ®"""
    name: str
    category: str  # 'technical', 'statistical', 'microstructure', 'interaction'
    market_dynamic: str  # 'trend', 'mean_reversion', 'volatility', 'volume'
    timeframe: str  # 'micro', 'meso', 'macro'
    math_type: str  # 'linear', 'nonlinear', 'frequency', 'information'
    factor_type: str  # 'risk', 'alpha', 'mixed'
    dependencies: List[str] = field(default_factory=list)
    compute_function: Optional[callable] = None
    version: str = "1.0"
    
@dataclass
class RegimeState:
    """å¸‚åœºçŠ¶æ€"""
    name: str  # 'trending', 'ranging', 'volatile'
    probability: float
    features: List[str]
    timestamp: pd.Timestamp

# ==================== ç‰¹å¾åŸºç±» ====================

class BaseFeature(ABC):
    """æ‰€æœ‰ç‰¹å¾çš„æŠ½è±¡åŸºç±»"""
    
    def __init__(self, metadata: FeatureMetadata):
        self.metadata = metadata
        self._cache = {}
        
    @abstractmethod
    def compute(self, data: pd.DataFrame, **params) -> pd.Series:
        """è®¡ç®—ç‰¹å¾å€¼"""
        pass
        
    def validate_input(self, data: pd.DataFrame) -> bool:
        """éªŒè¯è¾“å…¥æ•°æ®åŒ…å«å¿…éœ€åˆ—"""
        required_cols = self.metadata.dependencies
        return all(col in data.columns for col in required_cols)

# ==================== ç‰¹å¾æ³¨å†Œä¸­å¿ƒ ====================

class FeatureRegistry:
    """ç‰¹å¾ä¸­å¤®æ³¨å†Œè¡¨"""
    
    def __init__(self):
        self._features: Dict[str, BaseFeature] = {}
        self._metadata: Dict[str, FeatureMetadata] = {}
        self._hierarchy = self._initialize_hierarchy()
        
    def _initialize_hierarchy(self) -> Dict:
        """åˆå§‹åŒ–4å±‚ç‰¹å¾å±‚æ¬¡ç»“æ„"""
        return {
            'market_dynamics': {
                'trend': [],
                'mean_reversion': [],
                'volatility': [],
                'volume': []
            },
            'generation_method': {
                'technical': [],
                'statistical': [],
                'pattern': [],
                'microstructure': []
            },
            'timeframe': {
                'micro': [],  # åˆ†é’Ÿåˆ°å°æ—¶
                'meso': [],   # å¤©åˆ°å‘¨  
                'macro': []   # æœˆåˆ°å­£åº¦
            },
            'mathematical': {
                'linear': [],
                'nonlinear': [],
                'frequency': [],
                'information': []
            }
        }
        
    def register(self, feature: BaseFeature):
        """æ³¨å†Œç‰¹å¾åˆ°ç³»ç»Ÿ"""
        name = feature.metadata.name
        self._features[name] = feature
        self._metadata[name] = feature.metadata
        
        # æ›´æ–°å±‚æ¬¡ç»“æ„
        self._hierarchy['market_dynamics'][feature.metadata.market_dynamic].append(name)
        self._hierarchy['generation_method'][feature.metadata.category].append(name)
        self._hierarchy['timeframe'][feature.metadata.timeframe].append(name)
        self._hierarchy['mathematical'][feature.metadata.math_type].append(name)
        
    def get_feature(self, name: str) -> BaseFeature:
        """æŒ‰åç§°è·å–ç‰¹å¾"""
        return self._features.get(name)
        
    def get_features_by_category(self, category: str, subcategory: str) -> List[str]:
        """æŒ‰å±‚æ¬¡ç±»åˆ«è·å–ç‰¹å¾"""
        return self._hierarchy.get(category, {}).get(subcategory, [])

# ==================== æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ ====================

class TechnicalFeature(BaseFeature):
    """æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾å®ç°"""
    
    def compute(self, data: pd.DataFrame, **params) -> pd.Series:
        if not self.validate_input(data):
            raise ValueError(f"Missing dependencies for {self.metadata.name}")
            
        # ä½¿ç”¨ç¼“å­˜ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
        cache_key = f"{len(data)}_{data.index[-1]}"
        if cache_key in self._cache:
            return self._cache[cache_key]
            
        # æ ¹æ®ç‰¹å¾åç§°è®¡ç®—
        result = self._compute_specific(data, **params)
        
        # åº”ç”¨æ»åä»¥ä¿è¯å› æœæ€§
        result = result.shift(1)
        
        # ç¼“å­˜ç»“æœ
        self._cache[cache_key] = result
        return result
        
    def _compute_specific(self, data: pd.DataFrame, **params) -> pd.Series:
        """è®¡ç®—ç‰¹å®šæŠ€æœ¯æŒ‡æ ‡"""
        name = self.metadata.name
        
        if name == 'RSI_14':
            return talib.RSI(data['close'], timeperiod=14)
        elif name == 'MACD':
            macd, _, _ = talib.MACD(data['close'])
            return macd
        elif name == 'BB_width':
            upper, middle, lower = talib.BBANDS(data['close'], timeperiod=20)
            return upper - lower
        elif name == 'ADX_14':
            return talib.ADX(data['high'], data['low'], data['close'], timeperiod=14)
        elif name == 'ATR_14':
            return talib.ATR(data['high'], data['low'], data['close'], timeperiod=14)
        elif name == 'CCI_14':
            return talib.CCI(data['high'], data['low'], data['close'], timeperiod=14)
        elif name == 'STOCH_k':
            k, d = talib.STOCH(data['high'], data['low'], data['close'])
            return k
        elif name == 'OBV':
            return talib.OBV(data['close'], data['volume'])
        elif name == 'AD':
            return talib.AD(data['high'], data['low'], data['close'], data['volume'])
        elif name.startswith('SMA_'):
            period = int(name.split('_')[1])
            return talib.SMA(data['close'], timeperiod=period)
        elif name.startswith('EMA_'):
            period = int(name.split('_')[1])
            return talib.EMA(data['close'], timeperiod=period)
        elif name.startswith('close_ratio_SMA_'):
            period = int(name.split('_')[-1])
            sma = talib.SMA(data['close'], timeperiod=period)
            return data['close'] / sma
        else:
            raise NotImplementedError(f"Technical indicator {name} not implemented")

# ==================== ç»Ÿè®¡ç‰¹å¾ ====================

class StatisticalFeature(BaseFeature):
    """ç»Ÿè®¡ç‰¹å¾å®ç°"""
    
    def compute(self, data: pd.DataFrame, **params) -> pd.Series:
        if not self.validate_input(data):
            raise ValueError(f"Missing dependencies for {self.metadata.name}")
            
        name = self.metadata.name
        
        if name == 'z_score_20':
            close = data['close']
            rolling_mean = close.rolling(20).mean()
            rolling_std = close.rolling(20).std()
            z_score = (close - rolling_mean) / rolling_std
            return z_score.shift(1)
            
        elif name == 'realized_vol_20':
            returns = data['close'].pct_change()
            vol = returns.rolling(20).std() * np.sqrt(252)
            return vol.shift(1)
            
        elif name == 'realized_vol_5':
            returns = data['close'].pct_change()
            vol = returns.rolling(5).std() * np.sqrt(252)
            return vol.shift(1)
            
        elif name == 'max_drawdown_20':
            close = data['close']
            rolling_max = close.rolling(20).max()
            drawdown = (close / rolling_max - 1.0)
            return drawdown.rolling(20).min().shift(1)
            
        elif name == 'close_open_ratio':
            return (data['close'] / data['open']).shift(1)
            
        elif name == 'gap_size':
            return ((data['open'] - data['close'].shift(1)) / data['close'].shift(1)).shift(1)
            
        elif name == 'volume_ratio':
            vol_ma = data['volume'].rolling(10).mean()
            return (data['volume'] / vol_ma).shift(1)
            
        else:
            raise NotImplementedError(f"Statistical feature {name} not implemented")

# ==================== å¸‚åœºçŠ¶æ€æ£€æµ‹ ====================

class MarketRegimeDetector:
    """ä½¿ç”¨éšé©¬å°”ç§‘å¤«æ¨¡å‹æ£€æµ‹å¸‚åœºçŠ¶æ€"""
    
    def __init__(self, n_states: int = 3):
        self.n_states = n_states
        self.model = hmm.GaussianHMM(
            n_components=n_states,
            covariance_type="diag",
            n_iter=1000,  # å¢åŠ è¿­ä»£æ¬¡æ•°
            random_state=42,
            init_params='stm',  # åªåˆå§‹åŒ–å¼€å§‹æ¦‚ç‡ã€è½¬ç§»çŸ©é˜µå’Œå‡å€¼
            params='stmc'  # æ›´æ–°æ‰€æœ‰å‚æ•°
        )
        self.regime_names = {0: 'trending', 1: 'ranging', 2: 'volatile'}
        self.is_fitted = False
        
    def fit(self, data: pd.DataFrame):
        """æ‹ŸåˆHMMæ¨¡å‹"""
        features = self._extract_regime_features(data)
        if len(features) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®
            try:
                self.model.fit(features)
                self.is_fitted = True
            except:
                print("HMM fitting failed, using default parameters")
                self.is_fitted = False
        
    def detect_regime(self, data: pd.DataFrame) -> RegimeState:
        """æ£€æµ‹å½“å‰å¸‚åœºçŠ¶æ€"""
        if not self.is_fitted:
            self.fit(data)
            
        features = self._extract_regime_features(data)
        
        if self.is_fitted and len(features) > 0:
            try:
                states = self.model.predict(features)
                current_state = states[-1]
                
                # è·å–çŠ¶æ€æ¦‚ç‡
                probas = self.model.predict_proba(features)
                current_proba = probas[-1, current_state]
            except:
                # å¦‚æœé¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                current_state = 1  # ranging
                current_proba = 1.0
        else:
            # é»˜è®¤çŠ¶æ€
            current_state = 1  # ranging
            current_proba = 1.0
        
        # ä¸ºæ¯ä¸ªçŠ¶æ€å®šä¹‰æœ€ä¼˜ç‰¹å¾
        regime_features = {
            'trending': ['MACD', 'ADX_14', 'close_ratio_SMA_20', 'volume_ratio'],
            'ranging': ['RSI_14', 'CCI_14', 'BB_width', 'z_score_20'],
            'volatile': ['ATR_14', 'realized_vol_5', 'gap_size', 'TRANGE']
        }
        
        return RegimeState(
            name=self.regime_names[current_state],
            probability=current_proba,
            features=regime_features[self.regime_names[current_state]],
            timestamp=data.index[-1] if len(data) > 0 else pd.Timestamp.now()
        )
        
    def _extract_regime_features(self, data: pd.DataFrame) -> np.ndarray:
        """æå–ç”¨äºçŠ¶æ€æ£€æµ‹çš„ç‰¹å¾"""
        if len(data) < 50:
            return np.array([])
            
        returns = data['close'].pct_change()
        
        features = pd.DataFrame()
        features['return'] = returns
        features['volatility'] = returns.rolling(20).std()
        features['volume_change'] = data['volume'].pct_change()
        features['high_low_ratio'] = data['high'] / data['low'] - 1
        features['trend'] = data['close'].rolling(20).mean() / data['close'].rolling(50).mean() - 1
        
        # å¡«å……NaNå€¼
        features = features.fillna(method='ffill').fillna(0)
        
        return features.dropna().values

# ==================== åŠ¨æ€ç‰¹å¾é€‰æ‹© ====================

class DynamicFeatureSelector:
    """åŸºäºå¸‚åœºçŠ¶æ€åŠ¨æ€é€‰æ‹©ç‰¹å¾"""
    
    def __init__(self, registry: FeatureRegistry):
        self.registry = registry
        self.performance_history = {}
        self.regime_features = {
            'trending': {
                'primary': ['MACD', 'ADX_14', 'close_ratio_SMA_20'],
                'secondary': ['volume_ratio', 'OBV', 'AD']
            },
            'ranging': {
                'primary': ['RSI_14', 'CCI_14', 'BB_width'],
                'secondary': ['z_score_20', 'STOCH_k', 'close_open_ratio']
            },
            'volatile': {
                'primary': ['ATR_14', 'realized_vol_5', 'gap_size'],
                'secondary': ['NATR_14', 'TRANGE', 'max_drawdown_20']
            }
        }
        
    def select_features(self, 
                       regime: RegimeState,
                       data: pd.DataFrame,
                       n_features: int = 10) -> List[str]:
        """ä¸ºå½“å‰çŠ¶æ€é€‰æ‹©æœ€ä¼˜ç‰¹å¾"""
        
        # ä»çŠ¶æ€ç‰¹å®šç‰¹å¾å¼€å§‹
        primary = self.regime_features[regime.name]['primary'].copy()
        secondary = self.regime_features[regime.name]['secondary'].copy()
        
        # å¦‚æœçŠ¶æ€ç½®ä¿¡åº¦ä½ï¼Œæ·»åŠ å…¶ä»–çŠ¶æ€çš„ç‰¹å¾
        if regime.probability < 0.7:
            for other_regime in self.regime_features:
                if other_regime != regime.name:
                    secondary.extend(self.regime_features[other_regime]['primary'][:1])
                    
        # æŒ‰ä¿¡æ¯ç³»æ•°å¯¹ç‰¹å¾æ’åº
        all_features = list(set(primary + secondary))
        scores = self._calculate_feature_scores(all_features, data)
        
        # é€‰æ‹©å‰Nä¸ªç‰¹å¾
        ranked_features = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        selected = [f[0] for f in ranked_features[:n_features]]
        
        # ç¡®ä¿ç”¨æˆ·é€‰æ‹©çš„5ä¸ªç‰¹å¾æ€»æ˜¯åŒ…å«åœ¨å†…
        user_features = ['BB_width', 'gap_size', 'RSI_14', 'close_open_ratio', 'z_score_20']
        for feature in user_features:
            if feature not in selected:
                selected.append(feature)
                
        return selected[:n_features]
        
    def _calculate_feature_scores(self, 
                                 features: List[str],
                                 data: pd.DataFrame) -> Dict[str, float]:
        """è®¡ç®—ç‰¹å¾çš„ä¿¡æ¯ç³»æ•°"""
        scores = {}
        
        # ç®€å•çš„ICè®¡ç®—ï¼ˆä¸æœªæ¥æ”¶ç›Šçš„ç›¸å…³æ€§ï¼‰
        if 'close' in data.columns:
            forward_returns = data['close'].pct_change().shift(-1)
        else:
            return {f: 0 for f in features}
        
        for feature_name in features:
            try:
                feature = self.registry.get_feature(feature_name)
                if feature:
                    values = feature.compute(data)
                    # è®¡ç®—ç§©ç›¸å…³
                    ic = values.corr(forward_returns, method='spearman')
                    scores[feature_name] = abs(ic) if not np.isnan(ic) else 0
                else:
                    scores[feature_name] = 0
            except:
                scores[feature_name] = 0
                
        return scores

# ==================== ç‰¹å¾æ­£äº¤åŒ– ====================

class OrthogonalizationProcessor:
    """é€šè¿‡æ­£äº¤åŒ–æ¶ˆé™¤å†—ä½™"""
    
    def __init__(self, correlation_threshold: float = 0.85):
        self.correlation_threshold = correlation_threshold
        self.pca = PCA(n_components=0.95)
        self.scaler = StandardScaler()
        self.selected_features = None
        self.feature_mapping = None
        
    def fit_transform(self, feature_matrix: pd.DataFrame) -> pd.DataFrame:
        """æ‹Ÿåˆå¹¶è½¬æ¢ç‰¹å¾åˆ°æ­£äº¤ç©ºé—´"""
        
        if len(feature_matrix.columns) < 2:
            return feature_matrix
            
        # ç§»é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾
        reduced_features = self._remove_correlated(feature_matrix)
        
        if len(reduced_features.columns) < 2:
            return reduced_features
            
        # æ ‡å‡†åŒ–
        scaled = self.scaler.fit_transform(reduced_features)
        
        # åº”ç”¨PCA
        components = self.pca.fit_transform(scaled)
        
        # åˆ›å»ºå¸¦ç»„ä»¶åç§°çš„DataFrame
        component_names = [f'PC{i+1}' for i in range(components.shape[1])]
        result = pd.DataFrame(
            components,
            index=feature_matrix.index,
            columns=component_names
        )
        
        # å­˜å‚¨æ˜ å°„ä»¥ä¿æŒå¯è§£é‡Šæ€§
        self.feature_mapping = pd.DataFrame(
            self.pca.components_,
            columns=reduced_features.columns,
            index=component_names
        )
        
        return result
        
    def _remove_correlated(self, feature_matrix: pd.DataFrame) -> pd.DataFrame:
        """ç§»é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾"""
        # è®¡ç®—ç›¸å…³çŸ©é˜µ
        corr_matrix = feature_matrix.corr().abs()
        
        # æ‰¾åˆ°è¦ç§»é™¤çš„ç‰¹å¾
        upper_tri = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        
        to_drop = [column for column in upper_tri.columns 
                  if any(upper_tri[column] > self.correlation_threshold)]
        
        self.selected_features = [col for col in feature_matrix.columns 
                                 if col not in to_drop]
        
        return feature_matrix[self.selected_features]

# ==================== å¾®è§‚ç»“æ„å¢å¼º ====================

class MicrostructureEnhancer:
    """ä½¿ç”¨å¸‚åœºå¾®è§‚ç»“æ„ä¿¡æ¯å¢å¼ºç‰¹å¾"""
    
    def __init__(self):
        self.enhancements = {
            'spread_adjusted': self._spread_adjust,
            'volume_weighted': self._volume_weight,
            'order_flow': self._order_flow_enhance
        }
        
    def enhance_features(self, 
                        features: pd.DataFrame,
                        original_data: pd.DataFrame = None,
                        microstructure_data: Optional[pd.DataFrame] = None) -> pd.DataFrame:
        """ä½¿ç”¨å¾®è§‚ç»“æ„ä¿¡æ¯å¢å¼ºä¼ ç»Ÿç‰¹å¾"""
        
        enhanced = features.copy()
        
        if microstructure_data is None:
            # ä»åŸå§‹æ•°æ®ç”Ÿæˆåˆæˆå¾®è§‚ç»“æ„ç‰¹å¾
            if original_data is not None:
                microstructure_data = self._generate_synthetic_microstructure(original_data)
            else:
                # å¦‚æœæ²¡æœ‰åŸå§‹æ•°æ®ï¼Œè¿”å›åŸå§‹ç‰¹å¾
                return enhanced
                
        # åº”ç”¨å¢å¼º
        for feature_name in features.columns:
            if 'RSI' in feature_name:
                enhanced[f'{feature_name}_flow'] = self._order_flow_enhance(
                    features[feature_name], microstructure_data
                )
            elif 'BB' in feature_name:
                enhanced[f'{feature_name}_spread'] = self._spread_adjust(
                    features[feature_name], microstructure_data
                )
                
        return enhanced
        
    def _spread_adjust(self, feature: pd.Series, micro_data: pd.DataFrame) -> pd.Series:
        """æŒ‰ä¹°å–ä»·å·®è°ƒæ•´ç‰¹å¾"""
        if 'spread' in micro_data.columns and 'close' in micro_data.columns:
            spread_factor = 1 + micro_data['spread'] / micro_data['close']
            return feature * spread_factor
        return feature
        
    def _volume_weight(self, feature: pd.Series, micro_data: pd.DataFrame) -> pd.Series:
        """æŒ‰æˆäº¤é‡åŠ æƒç‰¹å¾"""
        if 'volume' in micro_data.columns:
            vol_weight = micro_data['volume'] / micro_data['volume'].rolling(20).mean()
            return feature * vol_weight.clip(0.5, 2.0)
        return feature
        
    def _order_flow_enhance(self, feature: pd.Series, micro_data: pd.DataFrame) -> pd.Series:
        """ä½¿ç”¨è®¢å•æµå¤±è¡¡å¢å¼º"""
        # ä»æˆäº¤é‡å’Œä»·æ ¼å˜åŒ–æ¨¡æ‹Ÿè®¢å•æµ
        if 'volume' in micro_data.columns and 'close' in micro_data.columns:
            price_change = micro_data['close'].pct_change()
            signed_volume = micro_data['volume'] * np.sign(price_change)
            flow_imbalance = signed_volume.rolling(10).sum() / micro_data['volume'].rolling(10).sum()
            return feature * (1 + flow_imbalance.fillna(0).clip(-0.5, 0.5))
        return feature
        
    def _generate_synthetic_microstructure(self, data: pd.DataFrame) -> pd.DataFrame:
        """åœ¨çœŸå®æ•°æ®ä¸å¯ç”¨æ—¶ç”Ÿæˆåˆæˆå¾®è§‚ç»“æ„æ•°æ®"""
        micro = pd.DataFrame(index=data.index)
        
        # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—
        if 'close' in data.columns:
            micro['close'] = data['close']
        
        if 'volume' in data.columns:
            micro['volume'] = data['volume']
            
        # åŸºäºæ³¢åŠ¨ç‡çš„åˆæˆä»·å·®
        if 'close' in data.columns:
            returns = data['close'].pct_change()
            vol = returns.rolling(20).std()
            micro['spread'] = vol * 0.001  # ç®€å•çš„ä»·å·®æ¨¡å‹
        else:
            micro['spread'] = 0.001
            
        return micro

# ==================== ç‰¹å¾è´¨é‡ç›‘æ§ ====================

class FeatureQualityMonitor:
    """ç›‘æ§ç‰¹å¾è´¨é‡å’Œæ€§èƒ½"""
    
    def __init__(self):
        self.metrics_history = {}
        self.alert_thresholds = {
            'ic': 0.01,           # æœ€å°ä¿¡æ¯ç³»æ•°
            'stability': 0.5,     # æœ€å°ç¨³å®šæ€§å¾—åˆ†
            'decay_rate': 0.3,    # æœ€å¤§æœˆåº¦è¡°å‡
            'coverage': 0.95      # æœ€å°æ•°æ®è¦†ç›–ç‡
        }
        
    def evaluate_feature(self, 
                        feature_name: str,
                        feature_values: pd.Series,
                        target: pd.Series) -> Dict[str, float]:
        """è¯„ä¼°ç‰¹å¾è´¨é‡æŒ‡æ ‡"""
        
        metrics = {}
        
        # ä¿¡æ¯ç³»æ•°(IC)
        ic = feature_values.corr(target, method='spearman')
        metrics['ic'] = ic if not np.isnan(ic) else 0
        
        # ç¨³å®šæ€§ï¼ˆæ»šåŠ¨ICç¨³å®šæ€§ï¼‰
        if len(feature_values) > 252:
            rolling_ic = pd.Series([
                feature_values.iloc[i:i+252].corr(target.iloc[i:i+252], method='spearman')
                for i in range(0, len(feature_values)-252, 21)
                if not feature_values.iloc[i:i+252].isna().all()
            ])
            metrics['stability'] = 1 - rolling_ic.std() if len(rolling_ic) > 1 else 0
        else:
            metrics['stability'] = 0
        
        # è¦†ç›–ç‡ï¼ˆéç¼ºå¤±æ¯”ç‡ï¼‰
        metrics['coverage'] = 1 - feature_values.isna().sum() / len(feature_values)
        
        # è¡°å‡ç‡ï¼ˆICè¶‹åŠ¿ï¼‰
        if feature_name in self.metrics_history:
            history = self.metrics_history[feature_name]
            if len(history) > 5:
                recent_ics = [h['ic'] for h in history[-5:]]
                decay = (recent_ics[0] - recent_ics[-1]) / (abs(recent_ics[0]) + 1e-6)
                metrics['decay_rate'] = max(0, decay)
            else:
                metrics['decay_rate'] = 0
        else:
            metrics['decay_rate'] = 0
            
        # å­˜å‚¨åˆ°å†å²è®°å½•
        if feature_name not in self.metrics_history:
            self.metrics_history[feature_name] = []
        self.metrics_history[feature_name].append(metrics)
        
        # æ£€æŸ¥è­¦æŠ¥
        self._check_alerts(feature_name, metrics)
        
        return metrics
        
    def _check_alerts(self, feature_name: str, metrics: Dict[str, float]):
        """æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦è§¦å‘è­¦æŠ¥"""
        alerts = []
        
        if metrics['ic'] < self.alert_thresholds['ic']:
            alerts.append(f"Low IC: {metrics['ic']:.4f}")
            
        if metrics['stability'] < self.alert_thresholds['stability']:
            alerts.append(f"Low stability: {metrics['stability']:.4f}")
            
        if metrics['decay_rate'] > self.alert_thresholds['decay_rate']:
            alerts.append(f"High decay rate: {metrics['decay_rate']:.4f}")
            
        if alerts:
            print(f"âš ï¸ Alerts for {feature_name}: {', '.join(alerts)}")

# ==================== ä¸»ç‰¹å¾ç³»ç»Ÿ ====================

class FeatureSystem:
    """åè°ƒæ‰€æœ‰ç»„ä»¶çš„ä¸»ç³»ç»Ÿ"""
    
    def __init__(self):
        self.registry = FeatureRegistry()
        self.regime_detector = MarketRegimeDetector()
        self.feature_selector = DynamicFeatureSelector(self.registry)
        self.orthogonalizer = OrthogonalizationProcessor()
        self.enhancer = MicrostructureEnhancer()
        self.monitor = FeatureQualityMonitor()
        
        # åˆå§‹åŒ–é»˜è®¤ç‰¹å¾
        self._initialize_default_features()
        
    def _initialize_default_features(self):
        """æ³¨å†Œä¸ç”¨æˆ·é€‰æ‹©åŒ¹é…çš„é»˜è®¤ç‰¹å¾"""
        
        # ç”¨æˆ·é€‰æ‹©çš„ç‰¹å¾
        selected_features = [
            ('BB_width', 'technical', 'volatility', 'meso', 'nonlinear', 'risk'),
            ('gap_size', 'statistical', 'volatility', 'micro', 'linear', 'alpha'),
            ('RSI_14', 'technical', 'mean_reversion', 'meso', 'nonlinear', 'alpha'),
            ('close_open_ratio', 'statistical', 'trend', 'micro', 'linear', 'alpha'),
            ('z_score_20', 'statistical', 'mean_reversion', 'meso', 'linear', 'alpha')
        ]
        
        # é¢å¤–çš„æœ‰ç”¨ç‰¹å¾
        additional_features = [
            ('MACD', 'technical', 'trend', 'meso', 'nonlinear', 'alpha'),
            ('ADX_14', 'technical', 'trend', 'meso', 'nonlinear', 'risk'),
            ('ATR_14', 'technical', 'volatility', 'meso', 'linear', 'risk'),
            ('CCI_14', 'technical', 'mean_reversion', 'meso', 'nonlinear', 'alpha'),
            ('volume_ratio', 'statistical', 'volume', 'micro', 'linear', 'alpha'),
            ('realized_vol_5', 'statistical', 'volatility', 'micro', 'linear', 'risk'),
            ('realized_vol_20', 'statistical', 'volatility', 'meso', 'linear', 'risk'),
            ('max_drawdown_20', 'statistical', 'volatility', 'meso', 'nonlinear', 'risk'),
            ('close_ratio_SMA_20', 'technical', 'trend', 'meso', 'linear', 'alpha')
        ]
        
        all_features = selected_features + additional_features
        
        for name, category, dynamic, timeframe, math_type, factor_type in all_features:
            metadata = FeatureMetadata(
                name=name,
                category=category,
                market_dynamic=dynamic,
                timeframe=timeframe,
                math_type=math_type,
                factor_type=factor_type,
                dependencies=['open', 'high', 'low', 'close', 'volume']
            )
            
            if category == 'technical':
                feature = TechnicalFeature(metadata)
            else:
                feature = StatisticalFeature(metadata)
                
            self.registry.register(feature)
            
    def compute_features(self, 
                        data: pd.DataFrame,
                        feature_names: Optional[List[str]] = None,
                        regime_aware: bool = True,
                        enhance: bool = True,
                        orthogonalize: bool = True) -> pd.DataFrame:
        """è®¡ç®—å¸¦æœ‰æ‰€æœ‰å¢å¼ºçš„ç‰¹å¾"""
        
        # æ£€æµ‹å½“å‰çŠ¶æ€ï¼ˆå¦‚éœ€è¦ï¼‰
        if regime_aware:
            regime = self.regime_detector.detect_regime(data)
            if feature_names is None:
                feature_names = self.feature_selector.select_features(regime, data)
            print(f"ğŸ“Š Current regime: {regime.name} (confidence: {regime.probability:.2%})")
        else:
            if feature_names is None:
                # ä½¿ç”¨æ‰€æœ‰å·²æ³¨å†Œç‰¹å¾
                feature_names = list(self.registry._features.keys())
                
        # è®¡ç®—åŸºç¡€ç‰¹å¾
        features = pd.DataFrame(index=data.index)
        for feature_name in feature_names:
            feature_obj = self.registry.get_feature(feature_name)
            if feature_obj:
                try:
                    features[feature_name] = feature_obj.compute(data)
                except Exception as e:
                    print(f"âš ï¸ Error computing {feature_name}: {e}")
                    
        # ä½¿ç”¨å¾®è§‚ç»“æ„å¢å¼º
        if enhance:
            features = self.enhancer.enhance_features(features, original_data=data)
            
        # æ­£äº¤åŒ–
        if orthogonalize and len(features.columns) > 5:
            # ç§»é™¤NaNå€¼è¿›è¡Œæ­£äº¤åŒ–
            clean_features = features.dropna()
            if len(clean_features) > 10:
                orthogonal = self.orthogonalizer.fit_transform(clean_features)
                # ç»“åˆåŸå§‹å’Œæ­£äº¤ç‰¹å¾
                features = pd.concat([features, orthogonal], axis=1)
            
        return features
        
    def evaluate_features(self, features: pd.DataFrame, target: pd.Series) -> pd.DataFrame:
        """è¯„ä¼°æ‰€æœ‰ç‰¹å¾å¹¶è¿”å›è´¨é‡æŒ‡æ ‡"""
        
        results = []
        
        for feature_name in features.columns:
            if feature_name.startswith('PC'):  # è·³è¿‡ä¸»æˆåˆ†
                continue
                
            metrics = self.monitor.evaluate_feature(
                feature_name,
                features[feature_name],
                target
            )
            
            metrics['feature'] = feature_name
            results.append(metrics)
            
        return pd.DataFrame(results).set_index('feature')
        
    def get_feature_importance(self, 
                              features: pd.DataFrame,
                              target: pd.Series,
                              method: str = 'mutual_info') -> pd.Series:
        """è®¡ç®—ç‰¹å¾é‡è¦æ€§å¾—åˆ†"""
        
        # ç§»é™¤NaNå€¼
        mask = features.notna().all(axis=1) & target.notna()
        clean_features = features[mask]
        clean_target = target[mask]
        
        if len(clean_features) == 0:
            return pd.Series()
            
        if method == 'mutual_info':
            # ç¦»æ•£åŒ–ç›®æ ‡ä»¥è®¡ç®—äº’ä¿¡æ¯
            target_binary = (clean_target > clean_target.median()).astype(int)
            scores = mutual_info_classif(clean_features, target_binary)
            importance = pd.Series(scores, index=features.columns)
        elif method == 'correlation':
            importance = features.corrwith(target).abs()
        else:
            raise ValueError(f"Unknown method: {method}")
            
        return importance.sort_values(ascending=False)
        
    def generate_report(self, data: pd.DataFrame) -> Dict:
        """ç”Ÿæˆç»¼åˆç‰¹å¾åˆ†ææŠ¥å‘Š"""
        
        report = {
            'timestamp': pd.Timestamp.now(),
            'data_shape': data.shape,
            'regime': self.regime_detector.detect_regime(data),
            'feature_count': len(self.registry._features),
            'hierarchy': self.registry._hierarchy
        }
        
        # è®¡ç®—æ‰€æœ‰ç‰¹å¾
        features = self.compute_features(data, regime_aware=True)
        report['computed_features'] = list(features.columns)
        
        # è®¡ç®—ç‰¹å¾é‡è¦æ€§
        if 'future_return' in data.columns:
            target = data['future_return']
            importance = self.get_feature_importance(features, target)
            report['feature_importance'] = importance.to_dict()
            
            # è¯„ä¼°è´¨é‡
            quality = self.evaluate_features(features, target)
            report['feature_quality'] = quality.to_dict()
            
        return report

def quick_start():
    """å¿«é€Ÿå¼€å§‹ä½¿ç”¨æ¡†æ¶"""
    
    # 1. åŸºæœ¬ä½¿ç”¨ - ä½¿ç”¨æ‚¨çš„5ä¸ªé€‰å®šç‰¹å¾
    system = FeatureSystem()
    
    # åŠ è½½æ•°æ®
    data = pd.read_csv('SPY_10Y_daily.csv')
    data.columns = [col.lower() for col in data.columns]
    
    if 'date' in data.columns:
        data['date'] = pd.to_datetime(data['date'])
        data.set_index('date', inplace=True)
    
    # ä»…è®¡ç®—æ‚¨é€‰æ‹©çš„5ä¸ªç‰¹å¾
    selected_5 = ['BB_width', 'gap_size', 'RSI_14', 'close_open_ratio', 'z_score_20']
    features = system.compute_features(data, feature_names=selected_5, regime_aware=False)
    
    print("âœ… åŸºæœ¬ç‰¹å¾è®¡ç®—å®Œæˆ")
    print(f"Features shape: {features.shape}")
    print(f"Features: {features.columns.tolist()}")
    
    # 2. ä¸ç°æœ‰ä»£ç é›†æˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        # å°è¯•å¯¼å…¥æ‚¨åŸæœ‰çš„ç‰¹å¾ç”Ÿæˆå™¨
        from Feature_engineering import FeatureGenerator
        
        # ä½¿ç”¨åŸæœ‰çš„ç‰¹å¾ç”Ÿæˆå™¨
        legacy_features = FeatureGenerator.generate(data)
        
        # å°†åŸæœ‰ç‰¹å¾å’Œæ–°ç³»ç»Ÿç»“åˆ
        combined_features = pd.concat([features, legacy_features], axis=1)
        
        # ç§»é™¤é‡å¤åˆ—
        combined_features = combined_features.loc[:, ~combined_features.columns.duplicated()]
        
        print(f"\nâœ… ç»“åˆåçš„ç‰¹å¾æ•°é‡: {len(combined_features.columns)}")
        
        return combined_features
        
    except ImportError:
        print("\nğŸ’¡ æç¤º: åŸæœ‰çš„ Feature_engineering æ¨¡å—æœªæ‰¾åˆ°")
        print("   å¦‚éœ€é›†æˆåŸæœ‰ä»£ç ï¼Œè¯·å°†æ‚¨çš„ FeatureGenerator ç±»ä¿å­˜ä¸º Feature_engineering.py")
        print("   å½“å‰ç³»ç»Ÿå·²ç‹¬ç«‹è¿è¡Œï¼Œæ— éœ€ä¾èµ–åŸæœ‰ä»£ç ")
        
        # è¿”å›æ–°ç³»ç»Ÿè®¡ç®—çš„ç‰¹å¾
        return features

# ==================== å±•ç¤ºæ›´å¤šåŠŸèƒ½ ====================

def demonstrate_advanced_features():
    """å±•ç¤ºç³»ç»Ÿçš„é«˜çº§åŠŸèƒ½"""
    
    print("\n" + "="*60)
    print("ğŸ”¬ ç³»ç»Ÿé«˜çº§åŠŸèƒ½æ¼”ç¤º")
    print("="*60)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    system = FeatureSystem()
    
    # åŠ è½½æ•°æ®
    data = pd.read_csv('SPY_10Y_daily.csv')
    data.columns = [col.lower() for col in data.columns]
    
    if 'date' in data.columns:
        data['date'] = pd.to_datetime(data['date'])
        data.set_index('date', inplace=True)
    
    # å¦‚æœæ²¡æœ‰future_returnï¼Œåˆ›å»ºå®ƒ
    if 'future_return' not in data.columns:
        data['future_return'] = data['close'].pct_change().shift(-1)
    
    # 1. å±•ç¤ºå¸‚åœºçŠ¶æ€æ£€æµ‹
    print("\n1ï¸âƒ£ å¸‚åœºçŠ¶æ€æ£€æµ‹")
    regime = system.regime_detector.detect_regime(data)
    print(f"   å½“å‰å¸‚åœºçŠ¶æ€: {regime.name}")
    print(f"   ç½®ä¿¡åº¦: {regime.probability:.2%}")
    print(f"   æ¨èç‰¹å¾: {regime.features}")
    
    # 2. å±•ç¤ºåŠ¨æ€ç‰¹å¾é€‰æ‹©
    print("\n2ï¸âƒ£ åŠ¨æ€ç‰¹å¾é€‰æ‹©")
    selected_features = system.feature_selector.select_features(regime, data, n_features=10)
    print(f"   æ ¹æ®å½“å‰å¸‚åœºçŠ¶æ€é€‰æ‹©çš„ç‰¹å¾: {selected_features}")
    
    # 3. è®¡ç®—å®Œæ•´ç‰¹å¾é›†
    print("\n3ï¸âƒ£ è®¡ç®—å®Œæ•´ç‰¹å¾é›†ï¼ˆåŒ…å«å¢å¼ºå’Œæ­£äº¤åŒ–ï¼‰")
    features_full = system.compute_features(
        data,
        regime_aware=True,
        enhance=True,
        orthogonalize=True
    )
    print(f"   æ€»ç‰¹å¾æ•°: {len(features_full.columns)}")
    print(f"   åŒ…å«å¢å¼ºç‰¹å¾: {[col for col in features_full.columns if '_' in col and any(x in col for x in ['flow', 'spread'])]}")
    print(f"   åŒ…å«ä¸»æˆåˆ†: {[col for col in features_full.columns if col.startswith('PC')]}")
    
    # 4. ç‰¹å¾è´¨é‡è¯„ä¼°
    print("\n4ï¸âƒ£ ç‰¹å¾è´¨é‡è¯„ä¼°")
    target = data['future_return']
    quality_metrics = system.evaluate_features(features_full[selected_5], target)
    
    print("\n   ç‰¹å¾è´¨é‡æŒ‡æ ‡:")
    print(quality_metrics[['ic', 'stability', 'coverage']].round(4))
    
    # 5. ç‰¹å¾é‡è¦æ€§æ’åº
    print("\n5ï¸âƒ£ ç‰¹å¾é‡è¦æ€§æ’åºï¼ˆäº’ä¿¡æ¯æ–¹æ³•ï¼‰")
    importance = system.get_feature_importance(features_full, target, method='mutual_info')
    print("\n   å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾:")
    for i, (feat, score) in enumerate(importance.head(10).items(), 1):
        print(f"   {i}. {feat}: {score:.4f}")
    
    # 6. å±•ç¤ºç‰¹å¾å±‚æ¬¡ç»“æ„
    print("\n6ï¸âƒ£ ç‰¹å¾å±‚æ¬¡ç»“æ„")
    hierarchy = system.registry._hierarchy
    print("\n   æŒ‰å¸‚åœºåŠ¨æ€åˆ†ç±»:")
    for dynamic, features in hierarchy['market_dynamics'].items():
        if features:
            print(f"   - {dynamic}: {features[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ª
    
    return features_full

# ==================== å¦‚ä½•é›†æˆåŸæœ‰ä»£ç  ====================

def integration_guide():
    """å±•ç¤ºå¦‚ä½•é›†æˆåŸæœ‰çš„ FeatureGenerator"""
    
    print("\n" + "="*60)
    print("ğŸ“š é›†æˆæŒ‡å—ï¼šå¦‚ä½•æ•´åˆæ‚¨åŸæœ‰çš„ FeatureGenerator")
    print("="*60)
    
    print("""
    æ–¹æ³•1: åˆ›å»º Feature_engineering.py æ–‡ä»¶
    ----------------------------------------
    å°†æ‚¨åŸæœ‰çš„ FeatureGenerator ç±»ä¿å­˜ä¸º Feature_engineering.py:
    
    # Feature_engineering.py
    class FeatureGenerator:
        @staticmethod
        def generate(df):
            # æ‚¨åŸæœ‰çš„ç‰¹å¾ç”Ÿæˆä»£ç 
            factors = pd.DataFrame(index=df.index)
            # ... 
            return factors
    
    æ–¹æ³•2: ç›´æ¥åœ¨å½“å‰ä»£ç ä¸­ä½¿ç”¨
    ----------------------------------------
    """)
    
    # å±•ç¤ºå¦‚ä½•ç›´æ¥é›†æˆ
    print("ç¤ºä¾‹ä»£ç :")
    print("""
    # åˆ›å»ºä¸€ä¸ªåŒ…è£…å™¨æ¥ä½¿ç”¨æ‚¨åŸæœ‰çš„ç‰¹å¾é€»è¾‘
    class YourFeatureGenerator:
        @staticmethod
        def generate(df):
            factors = pd.DataFrame(index=df.index)
            o, h, l, c, v = df['open'], df['high'], df['low'], df['close'], df['volume']
            
            # æ‚¨åŸæœ‰çš„ç‰¹å¾è®¡ç®—é€»è¾‘
            factors['RSI_14'] = talib.RSI(c, 14).shift(1)
            factors['BB_width'] = (talib.BBANDS(c)[0] - talib.BBANDS(c)[2]).shift(1)
            # ... æ·»åŠ æ›´å¤šç‰¹å¾
            
            return factors
    
    # ä½¿ç”¨æ–°ç³»ç»Ÿå¢å¼ºåŸæœ‰ç‰¹å¾
    system = FeatureSystem()
    legacy_features = YourFeatureGenerator.generate(data)
    enhanced_features = system.enhancer.enhance_features(legacy_features, data)
    """)

def demonstrate_advanced_features():
    """å±•ç¤ºç³»ç»Ÿçš„é«˜çº§åŠŸèƒ½"""
    
    print("\n" + "="*60)
    print("ğŸ”¬ ç³»ç»Ÿé«˜çº§åŠŸèƒ½æ¼”ç¤º")
    print("="*60)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    system = FeatureSystem()
    
    # åŠ è½½æ•°æ®
    data = pd.read_csv('SPY_10Y_daily.csv')
    data.columns = [col.lower() for col in data.columns]
    
    if 'date' in data.columns:
        data['date'] = pd.to_datetime(data['date'])
        data.set_index('date', inplace=True)
    
    # å¦‚æœæ²¡æœ‰future_returnï¼Œåˆ›å»ºå®ƒ
    if 'future_return' not in data.columns:
        data['future_return'] = data['close'].pct_change().shift(-1)
    
    # å®šä¹‰ç”¨æˆ·é€‰æ‹©çš„5ä¸ªç‰¹å¾
    selected_5 = ['BB_width', 'gap_size', 'RSI_14', 'close_open_ratio', 'z_score_20']
    
    # 1. å±•ç¤ºå¸‚åœºçŠ¶æ€æ£€æµ‹
    print("\n1ï¸âƒ£ å¸‚åœºçŠ¶æ€æ£€æµ‹")
    regime = system.regime_detector.detect_regime(data)
    print(f"   å½“å‰å¸‚åœºçŠ¶æ€: {regime.name}")
    print(f"   ç½®ä¿¡åº¦: {regime.probability:.2%}")
    print(f"   æ¨èç‰¹å¾: {regime.features}")
    
    # 2. å±•ç¤ºåŠ¨æ€ç‰¹å¾é€‰æ‹©
    print("\n2ï¸âƒ£ åŠ¨æ€ç‰¹å¾é€‰æ‹©")
    selected_features = system.feature_selector.select_features(regime, data, n_features=10)
    print(f"   æ ¹æ®å½“å‰å¸‚åœºçŠ¶æ€é€‰æ‹©çš„ç‰¹å¾: {selected_features}")
    
    # 3. è®¡ç®—å®Œæ•´ç‰¹å¾é›†
    print("\n3ï¸âƒ£ è®¡ç®—å®Œæ•´ç‰¹å¾é›†ï¼ˆåŒ…å«å¢å¼ºå’Œæ­£äº¤åŒ–ï¼‰")
    features_full = system.compute_features(
        data,
        regime_aware=True,
        enhance=True,
        orthogonalize=True
    )
    print(f"   æ€»ç‰¹å¾æ•°: {len(features_full.columns)}")
    print(f"   åŒ…å«å¢å¼ºç‰¹å¾: {[col for col in features_full.columns if '_' in col and any(x in col for x in ['flow', 'spread'])]}")
    print(f"   åŒ…å«ä¸»æˆåˆ†: {[col for col in features_full.columns if col.startswith('PC')]}")
    
    # 4. ç‰¹å¾è´¨é‡è¯„ä¼°ï¼ˆä»…è¯„ä¼°å®é™…è®¡ç®—çš„ç‰¹å¾ï¼‰
    print("\n4ï¸âƒ£ ç‰¹å¾è´¨é‡è¯„ä¼°")
    target = data['future_return']
    
    # åªè¯„ä¼°å®é™…å­˜åœ¨çš„ç‰¹å¾
    available_features = [f for f in selected_5 if f in features_full.columns]
    if available_features:
        quality_metrics = system.evaluate_features(features_full[available_features], target)
        print("\n   ç‰¹å¾è´¨é‡æŒ‡æ ‡:")
        print(quality_metrics[['ic', 'stability', 'coverage']].round(4))
    else:
        print("   æ³¨æ„ï¼šç”±äºå¸‚åœºçŠ¶æ€ä¸ºtrendingï¼ŒæŸäº›mean_reversionç‰¹å¾å¯èƒ½æœªè¢«è®¡ç®—")
        
    # 5. å•ç‹¬è®¡ç®—ç”¨æˆ·é€‰æ‹©çš„5ä¸ªç‰¹å¾
    print("\n5ï¸âƒ£ è®¡ç®—ç”¨æˆ·æŒ‡å®šçš„5ä¸ªç‰¹å¾")
    user_features = system.compute_features(
        data,
        feature_names=selected_5,
        regime_aware=False,  # ä¸ä½¿ç”¨çŠ¶æ€æ„ŸçŸ¥ï¼Œç¡®ä¿è®¡ç®—æ‰€æœ‰æŒ‡å®šç‰¹å¾
        enhance=True,
        orthogonalize=False
    )
    print(f"   ç”¨æˆ·ç‰¹å¾æ•°: {len(user_features.columns)}")
    print(f"   ç‰¹å¾åˆ—è¡¨: {user_features.columns.tolist()}")
    
    # è¯„ä¼°ç”¨æˆ·ç‰¹å¾è´¨é‡
    if len(user_features.columns) > 0:
        quality_metrics_user = system.evaluate_features(user_features[selected_5], target)
        print("\n   ç”¨æˆ·ç‰¹å¾è´¨é‡æŒ‡æ ‡:")
        print(quality_metrics_user[['ic', 'stability', 'coverage']].round(4))
    
    # 6. ç‰¹å¾é‡è¦æ€§æ’åº
    print("\n6ï¸âƒ£ ç‰¹å¾é‡è¦æ€§æ’åºï¼ˆäº’ä¿¡æ¯æ–¹æ³•ï¼‰")
    importance = system.get_feature_importance(features_full, target, method='mutual_info')
    if len(importance) > 0:
        print("\n   å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾:")
        for i, (feat, score) in enumerate(importance.head(10).items(), 1):
            print(f"   {i}. {feat}: {score:.4f}")
    
    # 7. å±•ç¤ºç‰¹å¾å±‚æ¬¡ç»“æ„
    print("\n7ï¸âƒ£ ç‰¹å¾å±‚æ¬¡ç»“æ„")
    hierarchy = system.registry._hierarchy
    print("\n   æŒ‰å¸‚åœºåŠ¨æ€åˆ†ç±»:")
    for dynamic, features in hierarchy['market_dynamics'].items():
        if features:
            print(f"   - {dynamic}: {features[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ª
    
    # 8. æ¯”è¾ƒä¸åŒå¸‚åœºçŠ¶æ€ä¸‹çš„ç‰¹å¾é€‰æ‹©
    print("\n8ï¸âƒ£ ä¸åŒå¸‚åœºçŠ¶æ€ä¸‹çš„æœ€ä¼˜ç‰¹å¾")
    for state_name in ['trending', 'ranging', 'volatile']:
        regime_test = RegimeState(
            name=state_name,
            probability=1.0,
            features=[],
            timestamp=pd.Timestamp.now()
        )
        state_features = system.feature_selector.select_features(regime_test, data, n_features=5)
        print(f"   {state_name}: {state_features[:5]}")
    
    return features_full, user_features

# ä¿®æ”¹ä¸»å‡½æ•°ï¼Œæ›´å¥½åœ°å¤„ç†ç»“æœ
if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ç³»ç»ŸåŒ–ç‰¹å¾å·¥ç¨‹æ¡†æ¶...")
    
    try:
        # å¿«é€Ÿå¼€å§‹
        features_basic = quick_start()
        print("\nâœ… å¿«é€Ÿå¼€å§‹å®Œæˆ!")
        # å®šä¹‰é€‰æ‹©çš„5ä¸ªç‰¹å¾ï¼ˆå…¨å±€ä½¿ç”¨ï¼‰
        selected_5 = ['BB_width', 'gap_size', 'RSI_14', 'close_open_ratio', 'z_score_20']
        # å±•ç¤ºé«˜çº§åŠŸèƒ½
        features_advanced, user_features = demonstrate_advanced_features()
        print("\nâœ… é«˜çº§åŠŸèƒ½æ¼”ç¤ºå®Œæˆ!")
        
        # æ˜¾ç¤ºé›†æˆæŒ‡å—
        integration_guide()
        
        print("\n" + "="*60)
        print("ğŸ‰ ç³»ç»Ÿå·²æˆåŠŸè¿è¡Œï¼")
        print("="*60)
        
        # å±•ç¤ºæœ€ç»ˆç»“æœ
        print(f"\nğŸ“Š ç‰¹å¾è®¡ç®—ç»“æœæ±‡æ€»:")
        print(f"1. åŸºç¡€ç‰¹å¾ï¼ˆå¿«é€Ÿå¼€å§‹ï¼‰: {features_basic.shape[1]} ä¸ªç‰¹å¾")
        print(f"2. é«˜çº§ç‰¹å¾ï¼ˆçŠ¶æ€æ„ŸçŸ¥ï¼‰: {features_advanced.shape[1]} ä¸ªç‰¹å¾")
        print(f"3. ç”¨æˆ·æŒ‡å®šç‰¹å¾: {user_features.shape[1]} ä¸ªç‰¹å¾")
        
        # æ˜¾ç¤ºç‰¹å¾å¯¹æ¯”
        print(f"\nğŸ“ˆ ç‰¹å¾å¯¹æ¯”:")
        print(f"- åŸºç¡€ç‰¹å¾åŒ…å«: {features_basic.columns.tolist()}")
        print(f"- ç”¨æˆ·5ä¸ªæ ¸å¿ƒç‰¹å¾: {['BB_width', 'gap_size', 'RSI_14', 'close_open_ratio', 'z_score_20']}")
        print(f"- å¢å¼ºç‰¹å¾: {[col for col in features_basic.columns if 'flow' in col or 'spread' in col]}")
        print(f"- ä¸»æˆåˆ†: {[col for col in features_basic.columns if col.startswith('PC')]}")
        
        # ä¿å­˜ä¸åŒç‰ˆæœ¬çš„ç‰¹å¾
        features_basic.to_csv('features_basic.csv')
        features_advanced.to_csv('features_advanced.csv')
        user_features.to_csv('features_user_selected.csv')
        
        print(f"\nğŸ’¾ ç‰¹å¾å·²ä¿å­˜:")
        print(f"   - features_basic.csv (åŸºç¡€ç‰¹å¾)")
        print(f"   - features_advanced.csv (é«˜çº§ç‰¹å¾)")
        print(f"   - features_user_selected.csv (ç”¨æˆ·é€‰æ‹©çš„ç‰¹å¾)")
        
        # æä¾›ä½¿ç”¨å»ºè®®
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print(f"1. å¯¹äºç®€å•æ¨¡å‹ï¼Œä½¿ç”¨ features_basic.csv")
        print(f"2. å¯¹äºå¤æ‚ç­–ç•¥ï¼Œä½¿ç”¨ features_advanced.csv")
        print(f"3. å¯¹äºä¸åŸæœ‰æ¨¡å‹å¯¹æ¯”ï¼Œä½¿ç”¨ features_user_selected.csv")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()